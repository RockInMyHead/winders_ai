"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞ —Å TF-IDF —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import quote, urljoin, urlparse
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
import hashlib
import os
import time
import logging
from typing import List, Dict, Any
import re
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∞
CACHE_DIR = "cache"
os.makedirs(CACHE_DIR, exist_ok=True)

class AdvancedWebSearch:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        })
        
        # Configure proxy if enabled
        proxy_enabled = os.getenv("PROXY_ENABLED", "false").lower() == "true"
        if proxy_enabled:
            proxy_host = os.getenv("PROXY_HOST")
            proxy_port = os.getenv("PROXY_PORT")
            proxy_username = os.getenv("PROXY_USERNAME")
            proxy_password = os.getenv("PROXY_PASSWORD")
            
            if proxy_host and proxy_port:
                if proxy_username and proxy_password:
                    proxy_url = f"http://{proxy_username}:{proxy_password}@{proxy_host}:{proxy_port}"
                else:
                    proxy_url = f"http://{proxy_host}:{proxy_port}"
                
                self.session.proxies = {
                    "http": proxy_url,
                    "https": proxy_url,
                }
                logger.info(f"üåê AdvancedWebSearch: Proxy enabled {proxy_host}:{proxy_port}")
    
    def duck_search(self, query: str, max_results: int = 5) -> List[str]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo —Å fallback"""
        try:
            url = f"https://html.duckduckgo.com/html/?q={quote(query)}"
            logger.info(f"–ü–æ–∏—Å–∫ DuckDuckGo: {query}")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞ (—Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã)
            links = []
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è DuckDuckGo
            selectors = [
                ".result__url",
                ".result__a",
                ".result__title a",
                ".result a",
                "a[href*='http']"
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                for elem in elements:
                    href = elem.get("href")
                    if href and href.startswith("http"):
                        # –û—á–∏—â–∞–µ–º DuckDuckGo —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
                        if "/l/?uddg=" in href:
                            try:
                                from urllib.parse import unquote, parse_qs
                                parsed = parse_qs(href.split("?")[1])
                                if "uddg" in parsed:
                                    real_url = unquote(parsed["uddg"][0])
                                    if real_url.startswith("http"):
                                        href = real_url
                                    else:
                                        continue
                                else:
                                    continue
                            except:
                                continue
                        elif href.startswith("//"):
                            href = "https:" + href
                        
                        if href.startswith("http") and href not in links:
                            links.append(href)
                
                if links:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å—Å—ã–ª–∫–∏, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–æ–∏—Å–∫
                    break
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Å—ã–ª–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            if not links:
                logger.warning("DuckDuckGo –Ω–µ –≤–µ—Ä–Ω—É–ª —Å—Å—ã–ª–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return self._fallback_search(query, max_results)
            
            return links[:max_results]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ DuckDuckGo: {e}")
            return self._fallback_search(query, max_results)
    
    def _fallback_search(self, query: str, max_results: int = 5) -> List[str]:
        """Fallback –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            fallback_urls = [
                f"https://en.wikipedia.org/wiki/{quote(query.replace(' ', '_'))}",
                f"https://coinmarketcap.com/search/?q={quote(query)}",
                f"https://www.google.com/search?q={quote(query)}"
            ]
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ URL
            valid_urls = [url for url in fallback_urls if url.startswith("http")]
            logger.info(f"Fallback –ø–æ–∏—Å–∫: {len(valid_urls)} URL")
            
            return valid_urls[:max_results]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ fallback –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def fetch_text(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞: {url}")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup(["script", "style", "noscript", "nav", "footer", "header", "aside"]):
                element.decompose()
            
            # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ —Ä–µ–∫–ª–∞–º—ã –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
            for element in soup.find_all(class_=re.compile(r"(ad|advertisement|banner|menu|navigation|sidebar|footer|header)")):
                element.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            main_content = soup.find("main") or soup.find("article") or soup.find("div", class_=re.compile(r"(content|main|article)"))
            
            if main_content:
                text = main_content.get_text(separator="\n", strip=True)
            else:
                text = soup.get_text(separator="\n", strip=True)
            
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            text = '\n'.join(lines)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            return text[:5000]  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ª–∏–º–∏—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return ""
    
    def cache_path(self, key: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫—ç—à–∞"""
        return os.path.join(CACHE_DIR, hashlib.md5(key.encode()).hexdigest() + ".txt")
    
    def get_cached_or_fetch(self, url: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å"""
        cache_file = self.cache_path(url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à (–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω 24 —á–∞—Å–∞)
        if os.path.exists(cache_file):
            file_age = time.time() - os.path.getmtime(cache_file)
            if file_age < 86400:  # 24 —á–∞—Å–∞
                try:
                    with open(cache_file, "r", encoding="utf-8") as f:
                        content = f.read()
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞: {url}")
                    return content
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ {url}: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –∫—ç—à–∏—Ä—É–µ–º
        content = self.fetch_text(url)
        if content:
            try:
                with open(cache_file, "w", encoding="utf-8") as f:
                    f.write(content)
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫—ç—à: {url}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à {url}: {e}")
        
        return content
    
    def rank_contexts(self, query: str, docs: List[str]) -> List[str]:
        """–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (sklearn –æ—Ç–∫–ª—é—á–µ–Ω)"""
        # –í—Ä–µ–º–µ–Ω–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–µ–∑ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        logger.info("TF-IDF —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        return docs
    
    def search_and_analyze(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞"""
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫: {query}")
        
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫
        links = self.duck_search(query, max_results)
        if not links:
            return {
                "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞",
                "query": query,
                "results": []
            }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        raw_docs = []
        successful_links = []
        
        for link in links:
            content = self.get_cached_or_fetch(link)
            if content:
                raw_docs.append(content)
                successful_links.append(link)
        
        if not raw_docs:
            return {
                "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü",
                "query": query,
                "results": []
            }
        
        # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        ranked_docs = self.rank_contexts(query, raw_docs)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        results = []
        for i, (doc, link) in enumerate(zip(ranked_docs, successful_links)):
            results.append({
                "rank": i + 1,
                "url": link,
                "content": doc,
                "relevance_score": 1.0 - (i * 0.1)  # –ü—Ä–æ—Å—Ç–æ–π —Å–∫–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–Ω–≥–∞
            })
        
        logger.info(f"‚úÖ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        return {
            "query": query,
            "total_results": len(results),
            "results": results,
            "cache_hits": len([f for f in os.listdir(CACHE_DIR) if f.endswith('.txt')]) if os.path.exists(CACHE_DIR) else 0
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
advanced_search = AdvancedWebSearch()

def get_advanced_web_search(query: str, max_results: int = 5) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –≤–µ–±-–ø–æ–∏—Å–∫–∞"""
    return advanced_search.search_and_analyze(query, max_results)

def format_advanced_search_results(search_data: Dict[str, Any]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è AI"""
    if "error" in search_data:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {search_data['error']}"
    
    if not search_data.get("results"):
        return "‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
    
    formatted_text = f"üåê –ü–†–û–î–í–ò–ù–£–¢–´–ô –í–ï–ë-–ü–û–ò–°–ö –ü–û –ó–ê–ü–†–û–°–£ '{search_data['query']}':\n"
    formatted_text += f"üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {search_data['total_results']}\n"
    formatted_text += f"üíæ –ö—ç—à-—Ñ–∞–π–ª–æ–≤: {search_data.get('cache_hits', 0)}\n\n"
    
    for result in search_data["results"][:3]:  # –¢–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        formatted_text += f"### –†–µ–∑—É–ª—å—Ç–∞—Ç #{result['rank']}\n"
        formatted_text += f"üîó URL: {result['url']}\n"
        formatted_text += f"üìà –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['relevance_score']:.2f}\n"
        formatted_text += f"üìÑ –ö–æ–Ω—Ç–µ–Ω—Ç:\n{result['content'][:1000]}...\n\n"
    
    return formatted_text
